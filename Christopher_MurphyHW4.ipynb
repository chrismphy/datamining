{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l67ZWMU94Gt0"
      },
      "source": [
        "#Assignment 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzurviBMReuD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o50hwQjTRu5w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JxfRy1vqRs8k"
      },
      "outputs": [],
      "source": [
        "# Load your train data\n",
        "data = pd.read_csv('train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LlZrl-YtniQ6"
      },
      "outputs": [],
      "source": [
        "#load your val data\n",
        "val_data=pd.read_csv('val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "7cgQl1yy4seu",
        "outputId": "ad95f179-632c-4f21-fb25-37667a87bd27"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Load the training and validation datasets\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Preprocess the text for both training and validation data\n",
        "train_data['cleaned_text'] = train_data['abstract_text'].str.lower()\n",
        "val_data['cleaned_text'] = val_data['abstract_text'].str.lower()\n",
        "# Function to plot the class distribution\n",
        "def plot_class_distribution(train_distribution, val_distribution, class_names):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
        "\n",
        "    # Training class distribution plot\n",
        "    axes[0].bar(class_names, train_distribution)\n",
        "    axes[0].set_title('Training Set Class Distribution')\n",
        "    axes[0].set_ylabel('Proportion')\n",
        "    axes[0].set_xlabel('Classes')\n",
        "\n",
        "    # Validation class distribution plot\n",
        "    axes[1].bar(class_names, val_distribution)\n",
        "    axes[1].set_title('Validation Set Class Distribution')\n",
        "    axes[1].set_xlabel('Classes')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get class names sorted by index\n",
        "class_names = train_data['target'].value_counts().index.sort_values()\n",
        "\n",
        "# Plot the class distribution\n",
        "plot_class_distribution(\n",
        "    train_data['target'].value_counts(normalize=True).sort_index(),\n",
        "    val_data['target'].value_counts(normalize=True).sort_index(),\n",
        "    class_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "govOZtGRmneq"
      },
      "source": [
        "\n",
        "1. Load the training and validation datasets from the CSV files.\n",
        "2. Preprocess both datasets: clean the text, tokenize, and pad the sequences.\n",
        "Convert the target labels to a format suitable for training (e.g., one-hot encoding).\n",
        "3. Build the LSTM model (bdirectional LSTM Model with Dropout Layer)\n",
        "4. Compile the model and print the summary.\n",
        "5. Train the model using the training data and validate it on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqZsUe4R4Cpv",
        "outputId": "33503239-694c-4f0f-f41a-fb96d902b29b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 60s 21ms/step - loss: 0.8101 - accuracy: 0.7363 - val_loss: 0.5696 - val_accuracy: 0.7957\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 36s 14ms/step - loss: 0.5639 - accuracy: 0.8395 - val_loss: 0.5479 - val_accuracy: 0.8056\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 36s 14ms/step - loss: 0.4306 - accuracy: 0.8816 - val_loss: 0.6262 - val_accuracy: 0.7811\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 35s 14ms/step - loss: 0.3289 - accuracy: 0.9114 - val_loss: 0.7023 - val_accuracy: 0.7887\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 35s 14ms/step - loss: 0.2488 - accuracy: 0.9324 - val_loss: 0.7440 - val_accuracy: 0.7817\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "\n",
        "# Preprocess the text for both training and validation data\n",
        "train_data['cleaned_text'] = train_data['abstract_text'].str.lower()\n",
        "val_data['cleaned_text'] = val_data['abstract_text'].str.lower()\n",
        "\n",
        "# Tokenization and Padding Parameters\n",
        "vocab_size = 60000\n",
        "max_length = 64\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'     # pad or trunc\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Tokenization - Fit only on training data\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_data['cleaned_text'])\n",
        "\n",
        "# Convert training and validation texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['cleaned_text'])\n",
        "val_sequences = tokenizer.texts_to_sequences(val_data['cleaned_text'])\n",
        "\n",
        "# Pad the sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Label Encoding using OneHotEncoder for consistency in label dimensions\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoder.fit(train_data['target'].values.reshape(-1, 1))  # Fit to get the classes_\n",
        "train_labels = encoder.transform(train_data['target'].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val_data['target'].values.reshape(-1, 1))\n",
        "\n",
        "# Building the LSTM Model with a Dropout layer\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_labels.shape[1], activation='softmax')  # The output layer size based on unique labels\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Calculate class weights\n",
        "unique_classes = np.unique(train_data['target'])\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes,\n",
        "    y=train_data['target'].values\n",
        ")\n",
        "\n",
        "# Since we are working with one-hot encoding, we need to map the weights to the encoded labels.\n",
        "# First, get the unique classes from the encoder\n",
        "encoded_classes = encoder.categories_[0]\n",
        "\n",
        "# Now create the dictionary mapping the index of the class to the computed weight\n",
        "class_weight_dict = {i: class_weights[np.where(encoded_classes == unique_classes[i])[0][0]] for i in range(len(unique_classes))}\n",
        "\n",
        "# Use this 'class_weight_dict' when fitting the model\n",
        "history = model.fit(\n",
        "    train_padded,\n",
        "    train_labels,\n",
        "    epochs=5,\n",
        "    validation_data=(val_padded, val_labels),\n",
        "    class_weight=class_weight_dict\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gsXlYHHeRWc",
        "outputId": "5c62b654-b6a4-44a1-f7bc-099b445f9143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#save model\n",
        "\n",
        "model.save('q1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1rQU4kmnjq1u",
        "outputId": "21994e00-a95a-48f6-cee3-4e88bc80adbd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9d6590fd-9b83-49ef-8a66-66dc64d999c2\", \"q1.h5\", 92854920)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('q1.h5')  # For Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tyHYGwbPefx7"
      },
      "outputs": [],
      "source": [
        "#load model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('q1.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzXprxJ7Sk6T",
        "outputId": "0de669d2-d2c8-48cb-a962-0d937330ae4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "922/922 [==============================] - 2s 2ms/step - loss: 0.6938 - accuracy: 0.7774\n",
            "Test loss: 0.6937718987464905\n",
            "Test accuracy: 0.77740478515625\n"
          ]
        }
      ],
      "source": [
        "# Load the test dataset\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Preprocess the text for the test data\n",
        "test_data['cleaned_text'] = test_data['abstract_text'].str.lower()\n",
        "\n",
        "# Convert test text data to sequences and pad them\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['cleaned_text'])\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Encode the test labels\n",
        "test_labels = encoder.transform(test_data['target'].values.reshape(-1, 1))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_padded, test_labels)\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict the classes or probabilities on the test set\n",
        "test_predictions = model.predict(test_padded)\n",
        "\n",
        "# If your labels are one-hot encoded, convert predictions and labels back to class integers\n",
        "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
        "test_true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(test_true_classes, test_pred_classes, target_names=encoder.categories_[0])\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv9uQ4TU_RJz",
        "outputId": "8a20040c-8012-4da1-c453-48083032d4be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "922/922 [==============================] - 2s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  BACKGROUND       0.50      0.62      0.56      2663\n",
            " CONCLUSIONS       0.64      0.76      0.69      4426\n",
            "     METHODS       0.87      0.87      0.87      9751\n",
            "   OBJECTIVE       0.66      0.57      0.61      2377\n",
            "     RESULTS       0.89      0.79      0.84     10276\n",
            "\n",
            "    accuracy                           0.78     29493\n",
            "   macro avg       0.71      0.72      0.71     29493\n",
            "weighted avg       0.79      0.78      0.78     29493\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLzxkZIX7L78"
      },
      "source": [
        "Summary:\n",
        "I designed a model designed for classification task where the input is a sequence of text that has been tokenized and padded to a fixed length, and the output a probability distribution over possible classes. The bidirectional LSTMs allow the model to capture dependencies in both directions along the input sequnce, hopefully leading to a better performance than a unidirectional LSTM. The dropout layers were added to combat overfitting by randomly dropping out units in the netwrok during changing. Also, I have utilized class weighting to ensure that minority classes 'BACKGROUND' and 'OBJECTIVE' are given appropriate attention during training, even though it is important to note that the class distributions between the training and validation set are nearly identical.\n",
        "\n",
        "From the results, we can see that the model performs best on the METHODS and RESULTS classies, which could be due to the highest support (more training data). In contrast, the model struggles with the BACKGROUND and OBJECTIVE classes, which might be due to less training data, more complex distinguishing features or less distinctive patterns within these classes. Despite the class distribution being nearly identical between training and validation sets, the varying F1-scores suggests that certain classes may require further model tuning or additional data to improve model generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylESIpsfmhiH"
      },
      "source": [
        "#Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "e9g-2yU82PzO",
        "outputId": "d32045d5-e202-4999-90c6-627a27173345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PySoundFile\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from PySoundFile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->PySoundFile) (2.21)\n",
            "Installing collected packages: PySoundFile\n",
            "Successfully installed PySoundFile-0.9.0.post1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "soundfile"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install PySoundFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxwK1QC8_UeE",
        "outputId": "c7ba9c88-30d7-4d29-f17c-cb8e93248bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/edge-collected-gunshot-audio.zip\n",
            "replace /content/edge-collected-gunshot-audio/38s&ws_dot38_caliber/03fc4685-909e-42c5-aff0-f519f1d14b12_chan0_v0.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#unzip file\n",
        "!unzip '/content/edge-collected-gunshot-audio.zip' -d '/content/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsgdqYS0fC3c",
        "outputId": "bd97b808-2f71-4968-968e-39e1e06b46f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column names in the CSV file: ['filename', ' num_gunshots', ' gunshot_location (in seconds)', ' ', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24']\n",
            "Data from column ' gunshot_location (in seconds)':\n",
            "0       0.080998\n",
            "1       1.720635\n",
            "2       0.006281\n",
            "3       0.383537\n",
            "4       0.110748\n",
            "          ...   \n",
            "2143    1.750000\n",
            "2144    1.590839\n",
            "2145    1.858821\n",
            "2146    1.750000\n",
            "2147    1.750000\n",
            "Name:  gunshot_location (in seconds), Length: 2148, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load the CSV file\n",
        "import librosa\n",
        "import numpy as np\n",
        "metadata_csv_path = 'gunshot-audio-labels-only.csv'\n",
        "metadata_df = pd.read_csv(metadata_csv_path)\n",
        "\n",
        "# Print out all column names to check their exact format\n",
        "column_names = metadata_df.columns.tolist()\n",
        "print(\"Column names in the CSV file:\", column_names)\n",
        "\n",
        "# Try to match the expected column name with the actual column names, accounting for possible leading/trailing spaces\n",
        "for name in column_names:\n",
        "    if 'gunshot_location' in name.lower():\n",
        "        column_name = name\n",
        "        break\n",
        "else:\n",
        "    raise ValueError(\"No column contains 'gunshot_location' in its name.\")\n",
        "\n",
        "# Now you have the correct column name\n",
        "gunshot_locations = metadata_df[column_name]\n",
        "print(f\"Data from column '{column_name}':\")\n",
        "print(gunshot_locations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJG-48GOmj_O",
        "outputId": "8d1d9f75-e084-4b59-8f0d-eab7bdbcdb77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label dictionary: {'38s&ws_dot38_caliber': 0, 'glock_17_9mm_caliber': 1, 'remington_870_12_gauge': 2, 'ruger_ar_556_dot223_caliber': 3}\n",
            "Epoch 1/100\n",
            "145/145 [==============================] - 4s 13ms/step - loss: 1.5318 - accuracy: 0.4280 - val_loss: 1.2734 - val_accuracy: 0.4519\n",
            "Epoch 2/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1869 - accuracy: 0.4529 - val_loss: 1.1050 - val_accuracy: 0.4432\n",
            "Epoch 3/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1537 - accuracy: 0.4512 - val_loss: 2.8069 - val_accuracy: 0.2350\n",
            "Epoch 4/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1342 - accuracy: 0.4512 - val_loss: 5.8683 - val_accuracy: 0.2350\n",
            "Epoch 5/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1487 - accuracy: 0.4488 - val_loss: 2.9087 - val_accuracy: 0.3738\n",
            "Epoch 6/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1164 - accuracy: 0.4512 - val_loss: 1.0170 - val_accuracy: 0.4432\n",
            "Epoch 7/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1054 - accuracy: 0.4512 - val_loss: 4.1667 - val_accuracy: 0.5820\n",
            "Epoch 8/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1278 - accuracy: 0.4514 - val_loss: 1.2850 - val_accuracy: 0.4501\n",
            "Epoch 9/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1294 - accuracy: 0.4512 - val_loss: 7.2729 - val_accuracy: 0.3183\n",
            "Epoch 10/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1133 - accuracy: 0.4497 - val_loss: 18.5525 - val_accuracy: 0.1925\n",
            "Epoch 11/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1111 - accuracy: 0.4512 - val_loss: 19.3199 - val_accuracy: 0.1925\n",
            "Epoch 12/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.1122 - accuracy: 0.4512 - val_loss: 2.7323 - val_accuracy: 0.5976\n",
            "Epoch 13/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.0354 - accuracy: 0.4512 - val_loss: 49.4852 - val_accuracy: 0.1414\n",
            "Epoch 14/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.9948 - accuracy: 0.5217 - val_loss: 0.9106 - val_accuracy: 0.6036\n",
            "Epoch 15/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 1.0918 - accuracy: 0.4597 - val_loss: 1.7244 - val_accuracy: 0.5317\n",
            "Epoch 16/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.9992 - accuracy: 0.5291 - val_loss: 1.0318 - val_accuracy: 0.5455\n",
            "Epoch 17/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.9982 - accuracy: 0.5241 - val_loss: 1.2104 - val_accuracy: 0.4709\n",
            "Epoch 18/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.9647 - accuracy: 0.5369 - val_loss: 0.9492 - val_accuracy: 0.5672\n",
            "Epoch 19/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.9278 - accuracy: 0.5612 - val_loss: 0.9834 - val_accuracy: 0.5395\n",
            "Epoch 20/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.9022 - accuracy: 0.5829 - val_loss: 1.3393 - val_accuracy: 0.5577\n",
            "Epoch 21/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.8987 - accuracy: 0.5859 - val_loss: 1.8120 - val_accuracy: 0.5351\n",
            "Epoch 22/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.8685 - accuracy: 0.5978 - val_loss: 0.8477 - val_accuracy: 0.6349\n",
            "Epoch 23/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.8526 - accuracy: 0.6145 - val_loss: 0.7213 - val_accuracy: 0.6808\n",
            "Epoch 24/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.8292 - accuracy: 0.6262 - val_loss: 0.7941 - val_accuracy: 0.6618\n",
            "Epoch 25/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.7548 - accuracy: 0.6727 - val_loss: 21.5050 - val_accuracy: 0.3365\n",
            "Epoch 26/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.6416 - accuracy: 0.7297 - val_loss: 2.4644 - val_accuracy: 0.6895\n",
            "Epoch 27/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.5734 - accuracy: 0.7620 - val_loss: 0.4871 - val_accuracy: 0.8187\n",
            "Epoch 28/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.4616 - accuracy: 0.8063 - val_loss: 0.4540 - val_accuracy: 0.8526\n",
            "Epoch 29/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.3885 - accuracy: 0.8456 - val_loss: 0.2968 - val_accuracy: 0.8942\n",
            "Epoch 30/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.3122 - accuracy: 0.8742 - val_loss: 0.2443 - val_accuracy: 0.9063\n",
            "Epoch 31/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.3394 - accuracy: 0.8664 - val_loss: 0.2877 - val_accuracy: 0.8951\n",
            "Epoch 32/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.2357 - accuracy: 0.9046 - val_loss: 0.2369 - val_accuracy: 0.9133\n",
            "Epoch 33/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.2032 - accuracy: 0.9215 - val_loss: 0.2774 - val_accuracy: 0.9202\n",
            "Epoch 34/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1701 - accuracy: 0.9364 - val_loss: 0.1947 - val_accuracy: 0.9436\n",
            "Epoch 35/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1685 - accuracy: 0.9373 - val_loss: 0.1907 - val_accuracy: 0.9540\n",
            "Epoch 36/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1459 - accuracy: 0.9475 - val_loss: 0.2672 - val_accuracy: 0.9263\n",
            "Epoch 37/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1149 - accuracy: 0.9577 - val_loss: 0.1627 - val_accuracy: 0.9514\n",
            "Epoch 38/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.2543 - accuracy: 0.9095 - val_loss: 0.2542 - val_accuracy: 0.9341\n",
            "Epoch 39/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1348 - accuracy: 0.9510 - val_loss: 0.2290 - val_accuracy: 0.9324\n",
            "Epoch 40/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1122 - accuracy: 0.9603 - val_loss: 0.2310 - val_accuracy: 0.9532\n",
            "Epoch 41/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1756 - accuracy: 0.9458 - val_loss: 0.3175 - val_accuracy: 0.9219\n",
            "Epoch 42/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0837 - accuracy: 0.9731 - val_loss: 0.1820 - val_accuracy: 0.9497\n",
            "Epoch 43/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0649 - accuracy: 0.9790 - val_loss: 0.2191 - val_accuracy: 0.9532\n",
            "Epoch 44/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0613 - accuracy: 0.9803 - val_loss: 0.1863 - val_accuracy: 0.9488\n",
            "Epoch 45/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0632 - accuracy: 0.9796 - val_loss: 0.1970 - val_accuracy: 0.9566\n",
            "Epoch 46/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0605 - accuracy: 0.9813 - val_loss: 0.2037 - val_accuracy: 0.9532\n",
            "Epoch 47/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0413 - accuracy: 0.9874 - val_loss: 0.1515 - val_accuracy: 0.9610\n",
            "Epoch 48/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0499 - accuracy: 0.9818 - val_loss: 0.2064 - val_accuracy: 0.9497\n",
            "Epoch 49/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0545 - accuracy: 0.9829 - val_loss: 0.3254 - val_accuracy: 0.9436\n",
            "Epoch 50/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0515 - accuracy: 0.9844 - val_loss: 0.2350 - val_accuracy: 0.9471\n",
            "Epoch 51/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 0.3744 - val_accuracy: 0.9350\n",
            "Epoch 52/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0267 - accuracy: 0.9924 - val_loss: 0.1893 - val_accuracy: 0.9644\n",
            "Epoch 53/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.1928 - val_accuracy: 0.9644\n",
            "Epoch 54/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0180 - accuracy: 0.9931 - val_loss: 0.2506 - val_accuracy: 0.9627\n",
            "Epoch 55/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0346 - accuracy: 0.9909 - val_loss: 0.2504 - val_accuracy: 0.9523\n",
            "Epoch 56/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0498 - accuracy: 0.9848 - val_loss: 0.2971 - val_accuracy: 0.9445\n",
            "Epoch 57/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0438 - accuracy: 0.9874 - val_loss: 0.2242 - val_accuracy: 0.9514\n",
            "Epoch 58/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.2634 - val_accuracy: 0.9497\n",
            "Epoch 59/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0223 - accuracy: 0.9935 - val_loss: 0.2308 - val_accuracy: 0.9523\n",
            "Epoch 60/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1765 - accuracy: 0.9512 - val_loss: 0.2229 - val_accuracy: 0.9480\n",
            "Epoch 61/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0494 - accuracy: 0.9870 - val_loss: 0.2088 - val_accuracy: 0.9488\n",
            "Epoch 62/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1097 - accuracy: 0.9653 - val_loss: 0.1742 - val_accuracy: 0.9523\n",
            "Epoch 63/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0512 - accuracy: 0.9813 - val_loss: 0.2652 - val_accuracy: 0.9506\n",
            "Epoch 64/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0218 - accuracy: 0.9944 - val_loss: 0.2312 - val_accuracy: 0.9575\n",
            "Epoch 65/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0257 - accuracy: 0.9915 - val_loss: 0.2271 - val_accuracy: 0.9627\n",
            "Epoch 66/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0157 - accuracy: 0.9944 - val_loss: 0.1778 - val_accuracy: 0.9679\n",
            "Epoch 67/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0278 - accuracy: 0.9918 - val_loss: 0.2208 - val_accuracy: 0.9540\n",
            "Epoch 68/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0126 - accuracy: 0.9948 - val_loss: 0.1956 - val_accuracy: 0.9610\n",
            "Epoch 69/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.1776 - val_accuracy: 0.9584\n",
            "Epoch 70/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.1986 - val_accuracy: 0.9696\n",
            "Epoch 71/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0246 - accuracy: 0.9924 - val_loss: 0.1973 - val_accuracy: 0.9610\n",
            "Epoch 72/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.1954 - val_accuracy: 0.9653\n",
            "Epoch 73/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 0.2397 - val_accuracy: 0.9670\n",
            "Epoch 74/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.1865 - val_accuracy: 0.9662\n",
            "Epoch 75/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0296 - accuracy: 0.9915 - val_loss: 0.3848 - val_accuracy: 0.9245\n",
            "Epoch 76/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0259 - accuracy: 0.9931 - val_loss: 0.2560 - val_accuracy: 0.9601\n",
            "Epoch 77/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0426 - accuracy: 0.9909 - val_loss: 0.4393 - val_accuracy: 0.9176\n",
            "Epoch 78/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.2650 - val_accuracy: 0.9575\n",
            "Epoch 79/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.1069 - accuracy: 0.9705 - val_loss: 0.2724 - val_accuracy: 0.9488\n",
            "Epoch 80/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 0.2056 - val_accuracy: 0.9549\n",
            "Epoch 81/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0383 - accuracy: 0.9905 - val_loss: 0.1817 - val_accuracy: 0.9601\n",
            "Epoch 82/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0149 - accuracy: 0.9959 - val_loss: 0.2408 - val_accuracy: 0.9566\n",
            "Epoch 83/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.2717 - val_accuracy: 0.9523\n",
            "Epoch 84/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0275 - accuracy: 0.9922 - val_loss: 0.2190 - val_accuracy: 0.9523\n",
            "Epoch 85/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0221 - accuracy: 0.9952 - val_loss: 0.2275 - val_accuracy: 0.9618\n",
            "Epoch 86/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0207 - accuracy: 0.9937 - val_loss: 0.1828 - val_accuracy: 0.9618\n",
            "Epoch 87/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.2269 - val_accuracy: 0.9705\n",
            "Epoch 88/100\n",
            "145/145 [==============================] - 1s 9ms/step - loss: 0.0051 - accuracy: 0.9980 - val_loss: 0.2119 - val_accuracy: 0.9679\n",
            "Epoch 89/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.3318 - val_accuracy: 0.9514\n",
            "Epoch 90/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0411 - accuracy: 0.9883 - val_loss: 0.2371 - val_accuracy: 0.9558\n",
            "Epoch 91/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0679 - accuracy: 0.9783 - val_loss: 0.2718 - val_accuracy: 0.9488\n",
            "Epoch 92/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0776 - accuracy: 0.9761 - val_loss: 0.2136 - val_accuracy: 0.9419\n",
            "Epoch 93/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0379 - accuracy: 0.9892 - val_loss: 0.2019 - val_accuracy: 0.9523\n",
            "Epoch 94/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.2853 - val_accuracy: 0.9514\n",
            "Epoch 95/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0686 - accuracy: 0.9811 - val_loss: 0.2523 - val_accuracy: 0.9532\n",
            "Epoch 96/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0203 - accuracy: 0.9937 - val_loss: 0.1807 - val_accuracy: 0.9610\n",
            "Epoch 97/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0154 - accuracy: 0.9952 - val_loss: 0.1932 - val_accuracy: 0.9662\n",
            "Epoch 98/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.1978 - val_accuracy: 0.9670\n",
            "Epoch 99/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.2739 - val_accuracy: 0.9618\n",
            "Epoch 100/100\n",
            "145/145 [==============================] - 1s 10ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.2907 - val_accuracy: 0.9610\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import ast\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Helper functions\n",
        "def extract_uuid(filename):\n",
        "    return filename.split('_')[0]\n",
        "\n",
        "def pad2d(a, desired_size):\n",
        "    rows, cols = a.shape\n",
        "    padded_a = np.zeros((desired_size, desired_size))\n",
        "    rows_to_copy = min(rows, desired_size)\n",
        "    cols_to_copy = min(cols, desired_size)\n",
        "    padded_a[:rows_to_copy, :cols_to_copy] = a[:rows_to_copy, :cols_to_copy]\n",
        "    return padded_a\n",
        "\n",
        "def safe_eval_list(val):\n",
        "    try:\n",
        "        return ast.literal_eval(val)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return []\n",
        "\n",
        "def concatenate_timestamps(row):\n",
        "    timestamps = []\n",
        "    for i in range(2, len(row)):\n",
        "        if pd.notnull(row[i]):\n",
        "            if isinstance(row[i], str):\n",
        "                timestamps.extend(safe_eval_list(row[i]))\n",
        "            else:\n",
        "                timestamps.append(row[i])\n",
        "    return timestamps\n",
        "\n",
        "# Function to read data and extract features\n",
        "def read_data(folder_path, metadata_csv_path):\n",
        "    metadata_df = pd.read_csv(metadata_csv_path, converters={'gunshot_location_in_seconds': safe_eval_list})\n",
        "    labels = []\n",
        "    mfccs_list = []\n",
        "\n",
        "    for label in os.listdir(folder_path):\n",
        "        subfolder_path = os.path.join(folder_path, label)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            for file in os.listdir(subfolder_path):\n",
        "                file_path = os.path.join(subfolder_path, file)\n",
        "                if file.endswith('.wav'):\n",
        "                    uuid = extract_uuid(file)\n",
        "                    row = metadata_df[metadata_df['filename'].str.contains(uuid)].iloc[0]\n",
        "                    timestamps = concatenate_timestamps(row)\n",
        "\n",
        "                    y, sr = librosa.load(file_path, sr=22500)\n",
        "                    for timestamp in timestamps:\n",
        "                        start_sample = int(timestamp * sr)\n",
        "                        end_sample = start_sample + int(0.5 * sr)\n",
        "                        if start_sample < len(y) and end_sample <= len(y):\n",
        "                            gunshot_segment = y[start_sample:end_sample]\n",
        "                            mfccs = librosa.feature.mfcc(y=gunshot_segment, sr=sr, n_mfcc=40)\n",
        "                            padded_mfccs = pad2d(mfccs, 128)\n",
        "                            mfccs_list.append(padded_mfccs)\n",
        "                            labels.append(label)\n",
        "\n",
        "    return np.array(mfccs_list), labels\n",
        "\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Set the paths for the dataset and the metadata CSV\n",
        "folder_path = 'edge-collected-gunshot-audio'\n",
        "labels_csv_path ='gunshot-audio-labels-only.csv'\n",
        "\n",
        "# Read the data and labels\n",
        "mfccs, labels = read_data(folder_path, labels_csv_path)\n",
        "\n",
        "if not labels:\n",
        "    raise ValueError(\"Label array is empty. Ensure that the 'labels' list is being populated correctly.\")\n",
        "\n",
        "# Preprocess the data\n",
        "desired_size = 128\n",
        "mfccs = np.array([pad2d(m, desired_size) for m in mfccs])\n",
        "mfccs = np.expand_dims(mfccs, axis=-1)\n",
        "\n",
        "# Verify labels are not empty\n",
        "if not labels:\n",
        "    raise ValueError(\"Label array is empty. Ensure that the 'labels' list is being populated correctly.\")\n",
        "\n",
        "# Verify that labels are correctly mapped to indices\n",
        "label_dict = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
        "print(\"Label dictionary:\", label_dict)\n",
        "\n",
        "# Convert labels to indices using the label dictionary\n",
        "y_indices = [label_dict[label] for label in labels]\n",
        "\n",
        "# Verify that y_indices is not empty\n",
        "if not y_indices:\n",
        "    raise ValueError(\"y_indices array is empty. Ensure that labels are mapped to indices correctly.\")\n",
        "\n",
        "# Now convert label indices to one-hot encoded labels\n",
        "y = to_categorical(y_indices)\n",
        "\n",
        "# Verify that y is not empty and has the correct shape\n",
        "if y.size == 0 or y.shape[1] <= 1:\n",
        "    raise ValueError(f\"Problem with one-hot encoding. Check y: {y}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(mfccs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = X_train[0].shape\n",
        "num_classes = y.shape[1]\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_cnn_model(input_shape, num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the model\n",
        "model.save('gunshot_classification_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98JwUq-oxo_c",
        "outputId": "04867fb8-6479-4715-cc50-9bda87645ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - 0s 3ms/step\n",
            "Model Evaluation Metrics:\n",
            "Accuracy: 0.9609713790112749\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       222\n",
            "           1       0.92      0.98      0.95       257\n",
            "           2       1.00      0.85      0.92       163\n",
            "           3       0.96      0.98      0.97       511\n",
            "\n",
            "    accuracy                           0.96      1153\n",
            "   macro avg       0.97      0.95      0.96      1153\n",
            "weighted avg       0.96      0.96      0.96      1153\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('gunshot_classification_model')\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdt6-Jp57L7-"
      },
      "source": [
        "Summary:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3lqv0X17L7-"
      },
      "source": [
        "\n",
        "For the Classification report we have that\n",
        "'0'='ruger_ar_556_dot223_caliber'\n",
        "'1'='remington_870_12_gauge'\n",
        "'2'='glock_17_9mm_caliber'\n",
        "'3'='38s&ws_dot38_caliber'\n",
        "\n",
        "For the preprocesing step as we are processing the audiofiles we extract the Mel-frequency cepstral coefficients (MFCCs). These features are then padded to a uniform size for input into the CNN, and labels are converted from string format to one-hot encoded vectors. The CNN used includes multiple convolution layers with ReLU activation, max-pooling layers, batch normalization, a flattening layer, a dense layer, dropout for regularization, and a softmax output layer for classification. We split the data into testing and training sets. Next the CNN model is instantiated and compiled with the Adam optimized and categorical cross-entropy function, and trained with the training data.\n",
        "\n",
        "Upon evaluating the CNN model with the testing data, the classification report revealed high performance across the board. THe model achieved an accuracy of approximately 0.97, with precision, recall and F1-scores reflective of a robust ability to classify audio samples correctly. Particularly, the odel demonstrated perfect precision for class '2'='glock_17_9mm_caliber', indicating no false positive for this category. The recall was highest for '3'='38s&ws_dot38_caliber', suggesting that nearly all actual positives were correctly identified for this class. The macro and weighted averages for precision recall, and F1-score were consistent, at about 0.96 indicating a balanced performance across all classes and considering class support."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra Credit #1"
      ],
      "metadata": {
        "id": "INMvNEoiGPpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title:Measurements Analysis Classification, Detection of Gunshot and Gunshot like Sounds\n",
        "\n",
        "Author:Rajesh Baliram Singh, Hanqi Zhuang"
      ],
      "metadata": {
        "id": "_4F6J_aDMfMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question:\n",
        "The paper investigates the efficacy of using Random Forest classifiers within a machine learning model to detect gunshots acoustically, distinguishing them from nine other similar acoustic events.\n",
        "\n",
        "Methodology:\n",
        "The study standardizes a diverse dataset of ten audio classes to uniform 1-second clips and employs MFCCs for feature extraction, with a detailed preprocessing pipeline that includes windowing, Fourier transform, power spectrum computation, application of the Mel scale, and finally obtaining the MFCCs. The Random Forest Classifier is then trained on subsets of this data, utilizing multiple decision trees and majority voting for classification.\n",
        "\n",
        "Key Findings:\n",
        "The research concludes that Random Forest classifiers can accurately classify audio events, with feature importance evaluated using methods like MDI and SHAP to assess the contribution of each feature to the model's predictions, thereby enhancing the understanding and interpretability of the model's decision-making process."
      ],
      "metadata": {
        "id": "vvhlUsNbMWRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra Credit #2:"
      ],
      "metadata": {
        "id": "AybF2hYxGUtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title: Towards an Indoor Gunshot Detection and Notification System Using Deep Learning\n",
        "\n",
        "Author: Tareq Khan"
      ],
      "metadata": {
        "id": "k9LeTDaqNMcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question:\n",
        "The paper investigates the feasibility of using a deep learning-based CNN to detect indoor gunshot sounds from audio samples, focusing on the system's ability to differentiate gunshots from non-gunshot ambient noise.\n",
        "\n",
        "Methodology:\n",
        "The methodology involves training a CNN on 670,000 processed and augmented audio samples using MFCCs for feature extraction, followed by a validation phase for hyperparameter tuning and a testing phase on a separate dataset to ascertain the model's accuracy and efficiency.\n",
        "\n",
        "Key Findings:\n",
        "The key findings reveal that the CNN model achieves a 98% accuracy rate in identifying gunshot sounds and is effectively optimized for real-time audio analysis on the NVIDIA Jetson Nano, highlighting the system's precision and deployment readiness.\n"
      ],
      "metadata": {
        "id": "rQ4nDTxDG0aP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra Credit #3:"
      ],
      "metadata": {
        "id": "nKiKGx53GZMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title: Machine Learning Inspired Efficient Acoustic Gunshot Detection and Localization System\n",
        "\n",
        "Authors:Muhammad Salman Kabira, Junaid Mira, Caleb Rasconb,\n",
        "Muhammad Laiq Ur Rahman Shahida, Furqan Shaukata\n"
      ],
      "metadata": {
        "id": "U6K9YSl0NcKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Research Question:\n",
        "The article explores a machine learning system aimed at real-time detection and localization of gunshots to improve public safety responses, focusing on identifying the occurrence rather than the type of gunfire.\n",
        "\n",
        "Methodology:\n",
        "The approach utilizes acoustic feature extraction techniques like MFCC, GTCC, LPC, and spectral centroid, and implements machine learning classifiers such as bagged tree ensembles and support vector machines to recognize and localize gunshot sounds.\n",
        "\n",
        "Key Findings:\n",
        "The system successfully detects the presence of gunshots and locates their origin, prioritizing immediate recognition over firearm type differentiation, which is instrumental for rapid response in public safety applications."
      ],
      "metadata": {
        "id": "jJcW3XWvHECa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra Credit #4:"
      ],
      "metadata": {
        "id": "v66Nc5DfHGj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Title: Gun identification from gunshot audios for secure public places using transformer learning\n",
        "\n",
        "Authors: Rahul Nijhawan, SharikAliAnsari, Sunil Kumar, FawazAlassery,\n",
        "Sayed M. El‑kenawy"
      ],
      "metadata": {
        "id": "cfuLrHbZN1YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Research Question:\n",
        "The paper addresses the challenge of identifying the type of firearm used in a gunshot event based on the audio signature of the gunshot, aiming to classify sounds into categories such as rifle, handgun, or non-gunshot.\n",
        "\n",
        "Methodology:\n",
        "The research employs Mel-frequency cepstral coefficients (MFCCs) to characterize audio features and explores the effectiveness of convolutional neural networks (CNNs) versus transformer-based architectures in classifying 1-second audio clips of gunshots, which were uniformly standardized in the preprocessing phase.\n",
        "\n",
        "Key Findings:\n",
        "The study finds that transformer-based models, due to their ability to contextualize the entire audio clip, show promise in distinguishing between subtle differences in gunshot sounds, potentially outperforming traditional CNNs that focus on local sound features."
      ],
      "metadata": {
        "id": "gZUz60dZGZt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra Credit #5:"
      ],
      "metadata": {
        "id": "zBcAymniOXrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title: Alignment Based Matching Networks For One-Shot Classification and Open-Set Recognition\n",
        "\n",
        "Author: Paresh Malalur, Tommi Jaakkola"
      ],
      "metadata": {
        "id": "K0FsSSapObjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: The paper addresses the problem of one-shot classification and open-set recognition, focusing on how to effectively perform these tasks with minimal examples and ensuring the model remains interpretable.\n",
        "\n",
        "Methodology: The researchers propose Alignment-Based Matching Networks (ABM Nets), which utilize a two-tier matching model that first aligns images via learned contextual point matching and then uses this alignment to aid in classification. The methodology involves self-regularization for training and a novel approach to learn explicit alignments between images for improved interpretability and performance.\n",
        "\n",
        "Key Findings: ABM Nets achieved state-of-the-art performance on one-shot learning tasks and outperformed existing models in one-shot open-set recognition by maintaining high accuracy and F1 scores. The paper also shows that ABM Nets provide insights into the model's decision-making by analyzing point match likelihoods, which helps in understanding the semantic information captured by the network."
      ],
      "metadata": {
        "id": "1N3HJFi4P33r"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}